{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a37369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import postal\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from postal.expand import expand_address\n",
    "from postal.parser import parse_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64abd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('/home/james/PDS/client_data_feeds/realestate/airflow/.env')\n",
    "\n",
    "# Add airflow directory and parent directory to path (for pyRealtor)\n",
    "sys.path.insert(0, '/home/james/PDS/client_data_feeds/realestate/airflow')\n",
    "sys.path.insert(0, '/home/james/PDS/client_data_feeds/realestate')\n",
    "from include.db.connections import get_master_db_connection\n",
    "\n",
    "conn = get_master_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8c56689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34350/510714797.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  properties_df = pd.read_sql(\"SELECT * FROM properties\", conn)\n"
     ]
    }
   ],
   "source": [
    "# pull the entire properties table\n",
    "properties_df = pd.read_sql(\"SELECT * FROM properties\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6469e79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34350/118387109.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pools_df = pd.read_sql(\"SELECT * FROM pools\", conn)\n",
      "/tmp/ipykernel_34350/118387109.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  listings_df = pd.read_sql(\"SELECT * FROM listings\", conn)\n"
     ]
    }
   ],
   "source": [
    "pools_df = pd.read_sql(\"SELECT * FROM pools\", conn)\n",
    "listings_df = pd.read_sql(\"SELECT * FROM listings\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5617e1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22315"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(properties_df) # 23533\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c1ea674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "address_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "address_number",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lon",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "postal_code",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "street_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "municipality",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "province_state",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "geom",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "updated_at",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        }
       ],
       "ref": "489cc5c9-42da-4aee-9b06-862eccde681b",
       "rows": [
        [
         "0",
         "3aa7a580-f6eb-427a-a100-9e3243d196b4",
         "1635",
         "289",
         "Canada",
         "43.1355411",
         "-80.76640809999999",
         "N4S 4W8",
         "ingersoll avenue",
         "Woodstock",
         "Ontario",
         "0101000020E6100000ED388FD40C3154C0C6E1276959914540",
         "2026-01-19 18:35:22.398386+00:00"
        ],
        [
         "1",
         "2db2d4d8-db1a-472f-ad8e-7db7fdbe991b",
         "1637",
         "185",
         "Canada",
         "43.1351385",
         "-80.76477899999999",
         "N4S 6E7",
         "vansittart avenue",
         "Woodstock",
         "Ontario",
         "0101000020E6100000B96A9E23F23054C01AF7E6374C914540",
         "2026-01-19 18:35:22.398386+00:00"
        ],
        [
         "2",
         "54305c58-7ffb-4101-82a4-64d15f2c9d6b",
         "1643",
         "169",
         "Canada",
         "43.135483",
         "-80.7604313",
         "N4S 6K2",
         "graham street",
         "Woodstock",
         "Ontario",
         "0101000020E6100000B4160BE8AA3054C0FD14C78157914540",
         "2026-01-19 18:35:22.398386+00:00"
        ],
        [
         "3",
         "a0d6fd81-0d80-4d96-9012-8970470c1575",
         "1644",
         "406",
         "Canada",
         "43.1357368",
         "-80.7613119",
         "N4S 4X5",
         "ingersoll avenue",
         "Woodstock",
         "Ontario",
         "0101000020E61000008F238C55B93054C08D6ECED25F914540",
         "2026-01-19 18:35:22.398386+00:00"
        ],
        [
         "4",
         "45588391-35c9-4efe-81e1-48717ad3825b",
         "1649",
         "140",
         "Canada",
         "43.1345319",
         "-80.75952509999999",
         "N4S 6J9",
         "graham street",
         "Woodstock",
         "Ontario",
         "0101000020E61000006E3F2A0F9C3054C066625F5738914540",
         "2026-01-19 18:35:22.398386+00:00"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>address_id</th>\n",
       "      <th>address_number</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>street_name</th>\n",
       "      <th>municipality</th>\n",
       "      <th>province_state</th>\n",
       "      <th>geom</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3aa7a580-f6eb-427a-a100-9e3243d196b4</td>\n",
       "      <td>1635</td>\n",
       "      <td>289</td>\n",
       "      <td>Canada</td>\n",
       "      <td>43.135541</td>\n",
       "      <td>-80.766408</td>\n",
       "      <td>N4S 4W8</td>\n",
       "      <td>ingersoll avenue</td>\n",
       "      <td>Woodstock</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>0101000020E6100000ED388FD40C3154C0C6E127695991...</td>\n",
       "      <td>2026-01-19 18:35:22.398386+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2db2d4d8-db1a-472f-ad8e-7db7fdbe991b</td>\n",
       "      <td>1637</td>\n",
       "      <td>185</td>\n",
       "      <td>Canada</td>\n",
       "      <td>43.135138</td>\n",
       "      <td>-80.764779</td>\n",
       "      <td>N4S 6E7</td>\n",
       "      <td>vansittart avenue</td>\n",
       "      <td>Woodstock</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>0101000020E6100000B96A9E23F23054C01AF7E6374C91...</td>\n",
       "      <td>2026-01-19 18:35:22.398386+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54305c58-7ffb-4101-82a4-64d15f2c9d6b</td>\n",
       "      <td>1643</td>\n",
       "      <td>169</td>\n",
       "      <td>Canada</td>\n",
       "      <td>43.135483</td>\n",
       "      <td>-80.760431</td>\n",
       "      <td>N4S 6K2</td>\n",
       "      <td>graham street</td>\n",
       "      <td>Woodstock</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>0101000020E6100000B4160BE8AA3054C0FD14C7815791...</td>\n",
       "      <td>2026-01-19 18:35:22.398386+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a0d6fd81-0d80-4d96-9012-8970470c1575</td>\n",
       "      <td>1644</td>\n",
       "      <td>406</td>\n",
       "      <td>Canada</td>\n",
       "      <td>43.135737</td>\n",
       "      <td>-80.761312</td>\n",
       "      <td>N4S 4X5</td>\n",
       "      <td>ingersoll avenue</td>\n",
       "      <td>Woodstock</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>0101000020E61000008F238C55B93054C08D6ECED25F91...</td>\n",
       "      <td>2026-01-19 18:35:22.398386+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45588391-35c9-4efe-81e1-48717ad3825b</td>\n",
       "      <td>1649</td>\n",
       "      <td>140</td>\n",
       "      <td>Canada</td>\n",
       "      <td>43.134532</td>\n",
       "      <td>-80.759525</td>\n",
       "      <td>N4S 6J9</td>\n",
       "      <td>graham street</td>\n",
       "      <td>Woodstock</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>0101000020E61000006E3F2A0F9C3054C066625F573891...</td>\n",
       "      <td>2026-01-19 18:35:22.398386+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  address_id address_number country  \\\n",
       "0  3aa7a580-f6eb-427a-a100-9e3243d196b4        1635            289  Canada   \n",
       "1  2db2d4d8-db1a-472f-ad8e-7db7fdbe991b        1637            185  Canada   \n",
       "2  54305c58-7ffb-4101-82a4-64d15f2c9d6b        1643            169  Canada   \n",
       "3  a0d6fd81-0d80-4d96-9012-8970470c1575        1644            406  Canada   \n",
       "4  45588391-35c9-4efe-81e1-48717ad3825b        1649            140  Canada   \n",
       "\n",
       "         lat        lon postal_code        street_name municipality  \\\n",
       "0  43.135541 -80.766408     N4S 4W8   ingersoll avenue    Woodstock   \n",
       "1  43.135138 -80.764779     N4S 6E7  vansittart avenue    Woodstock   \n",
       "2  43.135483 -80.760431     N4S 6K2      graham street    Woodstock   \n",
       "3  43.135737 -80.761312     N4S 4X5   ingersoll avenue    Woodstock   \n",
       "4  43.134532 -80.759525     N4S 6J9      graham street    Woodstock   \n",
       "\n",
       "  province_state                                               geom  \\\n",
       "0        Ontario  0101000020E6100000ED388FD40C3154C0C6E127695991...   \n",
       "1        Ontario  0101000020E6100000B96A9E23F23054C01AF7E6374C91...   \n",
       "2        Ontario  0101000020E6100000B4160BE8AA3054C0FD14C7815791...   \n",
       "3        Ontario  0101000020E61000008F238C55B93054C08D6ECED25F91...   \n",
       "4        Ontario  0101000020E61000006E3F2A0F9C3054C066625F573891...   \n",
       "\n",
       "                        updated_at  \n",
       "0 2026-01-19 18:35:22.398386+00:00  \n",
       "1 2026-01-19 18:35:22.398386+00:00  \n",
       "2 2026-01-19 18:35:22.398386+00:00  \n",
       "3 2026-01-19 18:35:22.398386+00:00  \n",
       "4 2026-01-19 18:35:22.398386+00:00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e931d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to-lower on street_name column\n",
    "properties_df['street_name'] = properties_df['street_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd8a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_1_km(lat1, lon1, lat2, lon2):\n",
    "    from geopy.distance import geodesic\n",
    "    distance = geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "    return distance <= 1\n",
    "\n",
    "properties_df['full_address'] = properties_df['address_number'].astype(str) + ' ' + properties_df['street_name'] \n",
    "properties_df['full_address_expanded'] = properties_df['full_address'].apply(lambda x: expand_address(x) if expand_address(x) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5cf89b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "full_address_expanded",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "b8275fd6-0381-4005-a4ca-329012f1743a",
       "rows": [
        [
         "0",
         "['289 ingersoll avenue']"
        ],
        [
         "1",
         "['185 vansittart avenue']"
        ],
        [
         "2",
         "['169 graham street']"
        ],
        [
         "3",
         "['406 ingersoll avenue']"
        ],
        [
         "4",
         "['140 graham street']"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "0     [289 ingersoll avenue]\n",
       "1    [185 vansittart avenue]\n",
       "2        [169 graham street]\n",
       "3     [406 ingersoll avenue]\n",
       "4        [140 graham street]\n",
       "Name: full_address_expanded, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties_df['full_address_expanded'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bf5ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EARTH_RADIUS_M = 6371000.0\n",
    "\n",
    "def _haversine_m(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Vectorized haversine distance in meters.\"\"\"\n",
    "    lat1 = np.deg2rad(lat1); lon1 = np.deg2rad(lon1)\n",
    "    lat2 = np.deg2rad(lat2); lon2 = np.deg2rad(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return 2.0 * EARTH_RADIUS_M * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def find_duplicate_pairs_fast(properties_df: pd.DataFrame, km: float = 1.0):\n",
    "    \"\"\"\n",
    "    Fast replacement for the O(n^2) loop:\n",
    "    - any overlap between full_address_expanded arrays\n",
    "    - within `km` kilometers\n",
    "    Returns list[(id_i, id_j)].\n",
    "    \"\"\"\n",
    "    df = properties_df[['id', 'lat', 'lon', 'full_address_expanded']].copy()\n",
    "\n",
    "    # Ensure list-like; drop empties early\n",
    "    df['full_address_expanded'] = df['full_address_expanded'].apply(lambda x: x if isinstance(x, (list, tuple, set)) else [])\n",
    "    df = df[df['full_address_expanded'].map(len) > 0].reset_index(drop=True)\n",
    "\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    # Create token -> row index (explode)\n",
    "    tokens = df[['id', 'full_address_expanded']].explode('full_address_expanded', ignore_index=True)\n",
    "    tokens = tokens.rename(columns={'full_address_expanded': 'token'})\n",
    "\n",
    "    # If tokens can be messy, normalize (optional):\n",
    "    # tokens['token'] = tokens['token'].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Self-join on token to generate candidate pairs (only rows sharing >=1 token)\n",
    "    merged = tokens.merge(tokens, on='token', suffixes=('_a', '_b'))\n",
    "\n",
    "    # Keep only distinct pairs (id_a < id_b)\n",
    "    merged = merged[merged['id_a'] < merged['id_b']]\n",
    "\n",
    "    # Deduplicate pairs that share multiple tokens\n",
    "    pairs = merged[['id_a', 'id_b']].drop_duplicates()\n",
    "\n",
    "    # Join coords for vectorized distance check\n",
    "    coords = df.set_index('id')[['lat', 'lon']]\n",
    "    pairs = pairs.join(coords, on='id_a').rename(columns={'lat': 'lat_a', 'lon': 'lon_a'})\n",
    "    pairs = pairs.join(coords, on='id_b').rename(columns={'lat': 'lat_b', 'lon': 'lon_b'})\n",
    "\n",
    "    # Fast bounding-box prefilter before haversine\n",
    "    # ~1 deg lat â‰ˆ 111.32 km; lon scale depends on latitude\n",
    "    max_km = float(km)\n",
    "    lat_eps = max_km / 111.32\n",
    "    lat_mean = np.deg2rad((pairs['lat_a'].to_numpy() + pairs['lat_b'].to_numpy()) / 2.0)\n",
    "    lon_eps = max_km / (111.32 * np.maximum(np.cos(lat_mean), 1e-12))\n",
    "\n",
    "    lat_a = pairs['lat_a'].to_numpy()\n",
    "    lon_a = pairs['lon_a'].to_numpy()\n",
    "    lat_b = pairs['lat_b'].to_numpy()\n",
    "    lon_b = pairs['lon_b'].to_numpy()\n",
    "\n",
    "    bbox_ok = (np.abs(lat_a - lat_b) <= lat_eps) & (np.abs(lon_a - lon_b) <= lon_eps)\n",
    "\n",
    "    if not np.any(bbox_ok):\n",
    "        return []\n",
    "\n",
    "    pairs2 = pairs.loc[bbox_ok].copy()\n",
    "\n",
    "    # Vectorized haversine for the remaining candidates\n",
    "    d_m = _haversine_m(\n",
    "        pairs2['lat_a'].to_numpy(),\n",
    "        pairs2['lon_a'].to_numpy(),\n",
    "        pairs2['lat_b'].to_numpy(),\n",
    "        pairs2['lon_b'].to_numpy(),\n",
    "    )\n",
    "\n",
    "    keep = d_m <= (max_km * 1000.0)\n",
    "    out = list(zip(pairs2.loc[keep, 'id_a'], pairs2.loc[keep, 'id_b']))\n",
    "    return out\n",
    "\n",
    "# usage:\n",
    "duplicate_pairs = find_duplicate_pairs_fast(properties_df, km=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99a93257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1242"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(duplicate_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bec489b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LISTINGS COLS: ['id', 'mls_id', 'property_address_id', 'bathrooms', 'bedrooms', 'date_collected', 'description', 'house_cat', 'is_removed', 'listing_address_number', 'price', 'removal_date', 'size_sqft', 'stories', 'created_at', 'updated_at']\n",
      "POOLS COLS: ['id', 'pool_id', 'property_id', 'pool_type', 'cover_type', 'filter_type', 'created_at', 'updated_at']\n"
     ]
    }
   ],
   "source": [
    "print(\"LISTINGS COLS:\", listings_df.columns.tolist())\n",
    "print(\"POOLS COLS:\", pools_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "118cd3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218, 1201, 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_drop_lists_from_duplicates(\n",
    "    properties_df: pd.DataFrame,\n",
    "    duplicate_pairs: list[tuple[int, int]],\n",
    "    pools_df: pd.DataFrame,\n",
    "    listings_df: pd.DataFrame,\n",
    "):\n",
    "    \"\"\"\n",
    "    Rules:\n",
    "      - If exactly one property in a duplicate set has ANY listing(s): keep that property.\n",
    "      - If both have listings: only dedupe if their listings are the SAME (same mls_id set).\n",
    "      - If listings are the same:\n",
    "          - if one has a pool and the other doesn't: keep the one WITH a pool\n",
    "          - if both have pools or both don't: keep lower property id (drop higher id)\n",
    "\n",
    "    Output:\n",
    "      dict with ids/rows to drop in each table:\n",
    "        - properties_drop_ids: property ids\n",
    "        - pools_drop_row_ids: pools table row ids\n",
    "        - listings_drop_row_ids: listings table row ids\n",
    "    \"\"\"\n",
    "\n",
    "    if properties_df.empty or not duplicate_pairs:\n",
    "        return {\n",
    "            \"properties_drop_ids\": [],\n",
    "            \"pools_drop_row_ids\": [],\n",
    "            \"listings_drop_row_ids\": [],\n",
    "        }\n",
    "\n",
    "    # --- expected columns (based on what you printed) ---\n",
    "    prop_pk = \"id\"\n",
    "    pools_fk = \"property_id\"\n",
    "    pools_pk = \"id\"\n",
    "    listings_fk = \"property_address_id\"\n",
    "    listings_pk = \"id\"\n",
    "    listing_identity = \"mls_id\"\n",
    "\n",
    "    prop_ids = set(properties_df[prop_pk].tolist())\n",
    "\n",
    "    # --- pool presence per property ---\n",
    "    pools_by_prop = pools_df.groupby(pools_fk)[pools_pk].apply(list).to_dict()\n",
    "    has_pool = {pid: bool(pools_by_prop.get(pid)) for pid in prop_ids}\n",
    "\n",
    "    # --- listing signature per property: frozenset of mls_id values ---\n",
    "    # Drop NaNs so \"no listing identity\" doesn't accidentally create signatures\n",
    "    listings_sig_by_prop = (\n",
    "        listings_df.dropna(subset=[listing_identity])\n",
    "        .groupby(listings_fk)[listing_identity]\n",
    "        .apply(lambda s: frozenset(s.astype(str).tolist()))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    def listing_sig(pid: int):\n",
    "        return listings_sig_by_prop.get(pid, None)\n",
    "\n",
    "    listing_sig_map = {pid: listing_sig(pid) for pid in prop_ids}\n",
    "\n",
    "    # --- build duplicate graph (handles transitive duplicates) ---\n",
    "    adj = defaultdict(set)\n",
    "    for a, b in duplicate_pairs:\n",
    "        if a in prop_ids and b in prop_ids and a != b:\n",
    "            adj[a].add(b)\n",
    "            adj[b].add(a)\n",
    "\n",
    "    # --- connected components ---\n",
    "    visited = set()\n",
    "    components = []\n",
    "    for pid in adj.keys():\n",
    "        if pid in visited:\n",
    "            continue\n",
    "        q = deque([pid])\n",
    "        visited.add(pid)\n",
    "        comp = []\n",
    "        while q:\n",
    "            x = q.popleft()\n",
    "            comp.append(x)\n",
    "            for y in adj[x]:\n",
    "                if y not in visited:\n",
    "                    visited.add(y)\n",
    "                    q.append(y)\n",
    "        components.append(comp)\n",
    "\n",
    "    # --- choose drops per component ---\n",
    "    properties_to_drop = set()\n",
    "\n",
    "    def pick_keeper(cands: list[int]) -> int:\n",
    "        # Prefer pool presence, then lowest property id\n",
    "        return sorted(cands, key=lambda pid: (0 if has_pool.get(pid, False) else 1, pid))[0]\n",
    "\n",
    "    for comp in components:\n",
    "        # Partition within component by listing signature\n",
    "        by_sig = defaultdict(list)\n",
    "        none_group = []\n",
    "        for pid in comp:\n",
    "            sig = listing_sig_map.get(pid)\n",
    "            if sig is None:\n",
    "                none_group.append(pid)\n",
    "            else:\n",
    "                by_sig[sig].append(pid)\n",
    "\n",
    "        if by_sig:\n",
    "            # If ANY listed property exists in the component: all non-listed lose\n",
    "            for pid in none_group:\n",
    "                properties_to_drop.add(pid)\n",
    "\n",
    "            # Only dedupe among properties that share the SAME listing signature (same mls_id set)\n",
    "            for sig, group in by_sig.items():\n",
    "                if len(group) <= 1:\n",
    "                    continue\n",
    "                keeper = pick_keeper(group)\n",
    "                for pid in group:\n",
    "                    if pid != keeper:\n",
    "                        properties_to_drop.add(pid)\n",
    "\n",
    "        else:\n",
    "            # No listings anywhere: dedupe whole component\n",
    "            if len(comp) > 1:\n",
    "                keeper = pick_keeper(comp)\n",
    "                for pid in comp:\n",
    "                    if pid != keeper:\n",
    "                        properties_to_drop.add(pid)\n",
    "\n",
    "    # --- derive row drops in pools and listings tables ---\n",
    "    pools_drop_rows = pools_df.loc[pools_df[pools_fk].isin(properties_to_drop), pools_pk].tolist()\n",
    "    listings_drop_rows = listings_df.loc[listings_df[listings_fk].isin(properties_to_drop), listings_pk].tolist()\n",
    "\n",
    "    return {\n",
    "        \"properties_drop_ids\": sorted(properties_to_drop),\n",
    "        \"pools_drop_row_ids\": sorted(pools_drop_rows),\n",
    "        \"listings_drop_row_ids\": sorted(listings_drop_rows),\n",
    "    }\n",
    "\n",
    "\n",
    "# usage:\n",
    "# drops = build_drop_lists_from_duplicates(properties_df, duplicate_pairs, pools_df, listings_df)\n",
    "# drops[\"properties_drop_ids\"], drops[\"pools_drop_row_ids\"], drops[\"listings_drop_row_ids\"]\n",
    "drops = build_drop_lists_from_duplicates(properties_df, duplicate_pairs, pools_df, listings_df)\n",
    "len(drops[\"properties_drop_ids\"]), len(drops[\"pools_drop_row_ids\"]), len(drops[\"listings_drop_row_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04efde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_rows(conn, drops: dict):\n",
    "    pool_ids = drops.get(\"pools_drop_row_ids\", [])\n",
    "    prop_ids = drops.get(\"properties_drop_ids\", [])\n",
    "    if not pool_ids and not prop_ids:\n",
    "        print(\"Nothing to drop.\")\n",
    "        return\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            # ---- delete pools first ----\n",
    "            if pool_ids:\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    DELETE FROM pools\n",
    "                    WHERE id = ANY(%s::uuid[])\n",
    "                    \"\"\",\n",
    "                    (pool_ids,),\n",
    "                )\n",
    "                print(f\"Deleted {cur.rowcount} rows from pools\")\n",
    "\n",
    "            # ---- delete properties ----\n",
    "            if prop_ids:\n",
    "                cur.execute(\n",
    "                    \"\"\"\n",
    "                    DELETE FROM properties\n",
    "                    WHERE id = ANY(%s::uuid[])\n",
    "                    \"\"\",\n",
    "                    (prop_ids,),\n",
    "                )\n",
    "                print(f\"Deleted {cur.rowcount} rows from properties\")\n",
    "\n",
    "            conn.commit()\n",
    "\n",
    "        except Exception:\n",
    "            conn.rollback()\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0ebd4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c684e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 rows from pools\n",
      "Deleted 0 rows from properties\n"
     ]
    }
   ],
   "source": [
    "conn = get_master_db_connection()\n",
    "drops = drop_duplicate_rows(\n",
    "   conn,drops\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
